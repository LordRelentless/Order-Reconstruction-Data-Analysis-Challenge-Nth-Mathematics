{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLvy436GWs6+JdlmOv5m0j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LordRelentless/Order-Reconstruction-Data-Analysis-Challenge-Nth-Mathematics/blob/main/Order_Reconstruction_Data_Analysis_Take_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-Evaluation With Nth Mathematics and Nth Grover Style Ordering corrections"
      ],
      "metadata": {
        "id": "VooT3RnpyU8j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68563cd7",
        "outputId": "cee07edc-c209-4f1d-8e81-375caff6d27a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read one of the files to inspect its columns\n",
        "try:\n",
        "    df_sample = pd.read_csv('/content/file_01.csv')\n",
        "    print(df_sample.columns)\n",
        "except FileNotFoundError:\n",
        "    print(\"file_01.csv not found. Please ensure the data files are in the /content directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the sample file: {e}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['v', 'zct'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready full pipeline: v/zct -> features -> anomaly clusters -> bell-curve order -> final submission\n",
        "import os, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "# ============================\n",
        "# Configuration\n",
        "# ============================\n",
        "DATA_DIR = \"/content\"       # expects file_01.csv ... file_53.csv with columns ['v','zct']\n",
        "FS = 93750.0                # acceleration sampling frequency (Hz)\n",
        "GEAR_RATIO = 5.095238095    # tachometer shaft to turbine shaft\n",
        "MEAN_TACH = 105.25          # Hz approx\n",
        "NOM_TURBINE_SPEED = MEAN_TACH * GEAR_RATIO  # ~536.27 Hz\n",
        "GEOMETRY_FACTORS = {\"cage\":0.43, \"ball\":7.05, \"inner\":10.78, \"outer\":8.22}\n",
        "FAULT_BANDS = {k: NOM_TURBINE_SPEED*v for k,v in GEOMETRY_FACTORS.items()}\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Plot directories\n",
        "PLOT_DIR_PRESENTED = os.path.join(DATA_DIR, \"plots_presented_order\")\n",
        "PLOT_DIR_BELL = os.path.join(DATA_DIR, \"plots_bell_order\")\n",
        "os.makedirs(PLOT_DIR_PRESENTED, exist_ok=True)\n",
        "os.makedirs(PLOT_DIR_BELL, exist_ok=True)\n",
        "\n",
        "# ============================\n",
        "# Robust helpers\n",
        "# ============================\n",
        "def mad(x):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    med = np.median(x)\n",
        "    return np.median(np.abs(x - med)) + 1e-12\n",
        "\n",
        "def zscore(x, robust=False):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    if robust:\n",
        "        mu = np.median(x)\n",
        "        sigma = 1.4826 * mad(x)\n",
        "    else:\n",
        "        mu = np.mean(x)\n",
        "        sigma = np.std(x) + 1e-12\n",
        "    return (x - mu) / sigma\n",
        "\n",
        "def gaussian_smooth(arr, window, sigma=None):\n",
        "    arr = np.asarray(arr, dtype=float)\n",
        "    w = int(max(3, window))\n",
        "    if sigma is None:\n",
        "        sigma = max(1.0, w / 5.0)\n",
        "    half = w // 2\n",
        "    x = np.arange(-half, half + 1, dtype=float)\n",
        "    ker = np.exp(-(x**2) / (2 * sigma**2))\n",
        "    ker = ker / (ker.sum() + 1e-12)\n",
        "    pad = np.pad(arr, (half, half), mode='edge')\n",
        "    sm = np.convolve(pad, ker, mode='valid')\n",
        "    return sm\n",
        "\n",
        "def smooth_centered(arr, window, method=\"gaussian\"):\n",
        "    if method == \"gaussian\":\n",
        "        return gaussian_smooth(arr, window)\n",
        "    else:\n",
        "        return pd.Series(arr).rolling(int(window), center=True, min_periods=1).mean().values\n",
        "\n",
        "# ============================\n",
        "# Loader and zct cleaning\n",
        "# ============================\n",
        "def load_files(data_dir=DATA_DIR):\n",
        "    files = sorted(glob.glob(os.path.join(data_dir,\"file_*.csv\")))\n",
        "    items = {}\n",
        "    for path in files:\n",
        "        base = os.path.basename(path)\n",
        "        m = re.match(r\"file_(\\d+)\\.csv\", base)\n",
        "        if not m:\n",
        "            continue\n",
        "        item_id = int(m.group(1))\n",
        "        try:\n",
        "            df = pd.read_csv(path)\n",
        "        except Exception as e:\n",
        "            print(f\"Skip {base}: read error {e}\")\n",
        "            continue\n",
        "        if not {\"v\",\"zct\"}.issubset(df.columns):\n",
        "            print(f\"Skip {base}: missing v/zct\")\n",
        "            continue\n",
        "        # numeric coercion\n",
        "        df = df[[\"v\",\"zct\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
        "        # drop v NaNs; keep zct for cleaning\n",
        "        df = df.dropna(subset=[\"v\"]).copy()\n",
        "        zct = df[\"zct\"].dropna().values\n",
        "        # clean zct: sort, unique, rolling median, enforce monotonicity\n",
        "        if len(zct) >= 2:\n",
        "            zct = np.sort(zct)\n",
        "            zct = np.unique(zct)\n",
        "            if len(zct) >= 5:\n",
        "                zct = pd.Series(zct).rolling(5, center=True, min_periods=1).median().values\n",
        "            diffs = np.diff(zct)\n",
        "            idx_keep = np.where(diffs > 0)[0]\n",
        "            zct_clean = [zct[0]]\n",
        "            for idx in idx_keep:\n",
        "                zct_clean.append(zct[idx+1])\n",
        "            zct = np.array(zct_clean, dtype=float)\n",
        "        if len(zct) < 2:\n",
        "            # fallback: simulate sparse cycle boundaries across the recording\n",
        "            # Use 1-second spacing as a crude fallback (FS known), up to signal length\n",
        "            n_samples = len(df[\"v\"])\n",
        "            approx_cycles = int(n_samples / FS) - 1\n",
        "            if approx_cycles <= 1:\n",
        "                print(f\"Skip {base}: insufficient zct and too few samples\")\n",
        "                continue\n",
        "            zct = np.linspace(0.0, approx_cycles, approx_cycles+1)\n",
        "        items[item_id] = {\"v\": df[\"v\"].values.astype(float), \"zct\": zct.astype(float), \"filename\": base}\n",
        "    print(f\"Loaded {len(items)} files\")\n",
        "    return items\n",
        "\n",
        "# ============================\n",
        "# Tachometer/turbine utilities\n",
        "# ============================\n",
        "def tachometer_frequency(zct):\n",
        "    delta = np.diff(zct)\n",
        "    delta = np.where(delta <= 0, np.nan, delta)\n",
        "    f = 1.0/(delta)\n",
        "    return f\n",
        "\n",
        "def turbine_speed(f_tach):\n",
        "    return f_tach * GEAR_RATIO\n",
        "\n",
        "# ============================\n",
        "# Band energy feature\n",
        "# ============================\n",
        "def band_energy(signal, fs, center, bw=50.0):\n",
        "    nyq = 0.5*fs\n",
        "    low_hz = max(1.0, center - bw)\n",
        "    high_hz = min(nyq - 1.0, center + bw)\n",
        "    if low_hz >= high_hz:\n",
        "        return 0.0\n",
        "    low = low_hz/nyq\n",
        "    high = high_hz/nyq\n",
        "    try:\n",
        "        b,a = butter(4,[low,high],btype='band')\n",
        "        filt = filtfilt(b,a,signal)\n",
        "        return float(np.mean(filt**2))\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# ============================\n",
        "# Cycle segmentation and engineered features\n",
        "# ============================\n",
        "def extract_cycle_features(v, zct, fs=FS):\n",
        "    v = np.asarray(v, dtype=float)\n",
        "    zct = np.asarray(zct, dtype=float)\n",
        "    delta = np.diff(zct)\n",
        "    f_tach_series = tachometer_frequency(zct)\n",
        "    turbine_series = turbine_speed(f_tach_series)\n",
        "    cycles = []\n",
        "    for i in range(len(zct)-1):\n",
        "        if i >= len(f_tach_series) or not np.isfinite(f_tach_series[i]) or delta[i] <= 0:\n",
        "            continue\n",
        "        start_f = zct[i] * fs\n",
        "        end_f   = zct[i+1] * fs\n",
        "        if not np.isfinite(start_f) or not np.isfinite(end_f):\n",
        "            continue\n",
        "        start = int(max(0, min(len(v)-1, np.floor(start_f))))\n",
        "        end   = int(max(0, min(len(v),   np.floor(end_f))))\n",
        "        if end <= start or (end - start) < 20:\n",
        "            continue\n",
        "        seg = v[start:end]\n",
        "        feats = {\n",
        "            \"cycle_index\": i+1,\n",
        "            \"rms\": float(np.sqrt(np.mean(seg**2))),\n",
        "            \"skew\": float(skew(seg)),\n",
        "            \"kurt\": float(kurtosis(seg)),\n",
        "            \"tach_mean\": float(f_tach_series[i]) if np.isfinite(f_tach_series[i]) else 0.0,\n",
        "            \"turbine_mean\": float(turbine_series[i]) if np.isfinite(turbine_series[i]) else 0.0,\n",
        "        }\n",
        "        feats[\"band_cage\"]  = band_energy(seg, fs, FAULT_BANDS[\"cage\"],  bw=40.0)\n",
        "        feats[\"band_ball\"]  = band_energy(seg, fs, FAULT_BANDS[\"ball\"],  bw=80.0)\n",
        "        feats[\"band_inner\"] = band_energy(seg, fs, FAULT_BANDS[\"inner\"], bw=80.0)\n",
        "        feats[\"band_outer\"] = band_energy(seg, fs, FAULT_BANDS[\"outer\"], bw=80.0)\n",
        "        cycles.append(feats)\n",
        "    return pd.DataFrame(cycles)\n",
        "\n",
        "def process_all(data_dir=DATA_DIR):\n",
        "    items = load_files(data_dir)\n",
        "    processed = {}\n",
        "    for item_id, obj in items.items():\n",
        "        feats = extract_cycle_features(obj[\"v\"], obj[\"zct\"], fs=FS)\n",
        "        processed[item_id] = feats\n",
        "        print(f\"Item {item_id:02d} ({obj['filename']}): {len(feats)} cycles extracted\")\n",
        "    return items, processed\n",
        "\n",
        "# ============================\n",
        "# Graphing: presented order and bell-curve order\n",
        "# ============================\n",
        "def plot_factors_presented(items, processed):\n",
        "    # Presented order: file_01 -> file_53 by numeric id\n",
        "    for item_id in sorted(items.keys()):\n",
        "        v = items[item_id][\"v\"]\n",
        "        zct = items[item_id][\"zct\"]\n",
        "        df = processed[item_id]\n",
        "        # figure with v and zct markers, cycle boundaries\n",
        "        fig, ax = plt.subplots(2, 1, figsize=(10,6), sharex=False)\n",
        "        ax[0].plot(v, color='steelblue', lw=0.8)\n",
        "        ax[0].set_title(f\"Item {item_id:02d} - v (acceleration)\")\n",
        "        # mark zct positions in sample index\n",
        "        zct_samples = (zct * FS).astype(int)\n",
        "        zct_samples = zct_samples[(zct_samples >= 0) & (zct_samples < len(v))]\n",
        "        ax[0].vlines(zct_samples, ymin=np.min(v), ymax=np.max(v), color='orange', alpha=0.3, lw=0.5)\n",
        "        # cycle index vs RMS\n",
        "        if not df.empty:\n",
        "            ax[1].plot(df[\"cycle_index\"], df[\"rms\"], 'o-', color='darkred', lw=1.0, ms=3)\n",
        "            ax[1].set_title(\"Cycle RMS vs cycle_index\")\n",
        "            ax[1].set_xlabel(\"cycle_index\")\n",
        "            ax[1].set_ylabel(\"RMS\")\n",
        "        else:\n",
        "            ax[1].text(0.5, 0.5, \"No cycles extracted\", ha='center', va='center')\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(os.path.join(PLOT_DIR_PRESENTED, f\"item_{item_id:02d}_presented.png\"))\n",
        "        plt.close(fig)\n",
        "\n",
        "def bell_curve_scalar(df):\n",
        "    # Use per-item scalar based on normalized mean RMS and band energies\n",
        "    if df.empty:\n",
        "        return 0.0\n",
        "    rms_mean = float(np.mean(df[\"rms\"]))\n",
        "    be_mean = float(np.mean(df[[\"band_cage\",\"band_ball\",\"band_inner\",\"band_outer\"]].sum(axis=1)))\n",
        "    # Normalize to approximate Gaussian across items later via z-scoring\n",
        "    return rms_mean + 0.2 * be_mean\n",
        "\n",
        "def build_bell_curve_order(processed):\n",
        "    # Compute scalar per item, then z-score across items\n",
        "    scores = {}\n",
        "    for item_id, df in processed.items():\n",
        "        scores[item_id] = bell_curve_scalar(df)\n",
        "    vals = np.array(list(scores.values()), dtype=float)\n",
        "    z = zscore(vals, robust=True)\n",
        "    # Construct an order that spreads items along the bell curve (ascending z)\n",
        "    order = [item for item, _ in sorted(scores.items(), key=lambda kv: kv[1])]\n",
        "    return order, scores, z\n",
        "\n",
        "def plot_factors_bell(items, processed, bell_order):\n",
        "    for item_id in bell_order:\n",
        "        v = items[item_id][\"v\"]\n",
        "        zct = items[item_id][\"zct\"]\n",
        "        df = processed[item_id]\n",
        "        fig, ax = plt.subplots(2, 1, figsize=(10,6), sharex=False)\n",
        "        ax[0].plot(v, color='seagreen', lw=0.8)\n",
        "        ax[0].set_title(f\"Item {item_id:02d} - v (acceleration) [bell-curve order]\")\n",
        "        zct_samples = (zct * FS).astype(int)\n",
        "        zct_samples = zct_samples[(zct_samples >= 0) & (zct_samples < len(v))]\n",
        "        ax[0].vlines(zct_samples, ymin=np.min(v), ymax=np.max(v), color='purple', alpha=0.3, lw=0.5)\n",
        "        if not df.empty:\n",
        "            ax[1].plot(df[\"cycle_index\"], df[\"rms\"], 'o-', color='navy', lw=1.0, ms=3)\n",
        "            ax[1].set_title(\"Cycle RMS vs cycle_index [bell-curve order]\")\n",
        "            ax[1].set_xlabel(\"cycle_index\")\n",
        "            ax[1].set_ylabel(\"RMS\")\n",
        "        else:\n",
        "            ax[1].text(0.5, 0.5, \"No cycles extracted\", ha='center', va='center')\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(os.path.join(PLOT_DIR_BELL, f\"item_{item_id:02d}_bell.png\"))\n",
        "        plt.close(fig)\n",
        "\n",
        "# ============================\n",
        "# Bearing-feature output (per item)\n",
        "# ============================\n",
        "def summarize_bearing_features(processed):\n",
        "    rows = []\n",
        "    for item_id, df in processed.items():\n",
        "        if df.empty:\n",
        "            rows.append({\"item_id\": item_id, \"rms_mean\": 0.0, \"skew_mean\": 0.0, \"kurt_mean\": 0.0,\n",
        "                         \"tach_mean\": 0.0, \"turbine_mean\": 0.0,\n",
        "                         \"band_cage_mean\": 0.0, \"band_ball_mean\": 0.0, \"band_inner_mean\": 0.0, \"band_outer_mean\": 0.0})\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"item_id\": item_id,\n",
        "            \"rms_mean\": float(np.mean(df[\"rms\"])),\n",
        "            \"skew_mean\": float(np.mean(df[\"skew\"])),\n",
        "            \"kurt_mean\": float(np.mean(df[\"kurt\"])),\n",
        "            \"tach_mean\": float(np.mean(df[\"tach_mean\"])),\n",
        "            \"turbine_mean\": float(np.mean(df[\"turbine_mean\"])),\n",
        "            \"band_cage_mean\": float(np.mean(df[\"band_cage\"])),\n",
        "            \"band_ball_mean\": float(np.mean(df[\"band_ball\"])),\n",
        "            \"band_inner_mean\": float(np.mean(df[\"band_inner\"])),\n",
        "            \"band_outer_mean\": float(np.mean(df[\"band_outer\"])),\n",
        "        })\n",
        "    bf = pd.DataFrame(rows).sort_values(\"item_id\")\n",
        "    bf.to_csv(os.path.join(DATA_DIR, \"bearing_features.csv\"), index=False)\n",
        "    return bf\n",
        "\n",
        "# ============================\n",
        "# Anomaly clustering and initiation indices (emulating 134288)\n",
        "# ============================\n",
        "def dynamic_window_candidates(n):\n",
        "    base = max(7, n // 30)\n",
        "    candidates = [\n",
        "        (base, int(base*2), int(base*3)),\n",
        "        (base+2, int(base*2)+4, int(base*3)+6),\n",
        "        (base+4, int(base*2)+2, int(base*3)+10),\n",
        "        (base, int(base*2)+6, int(base*3)+12),\n",
        "        (max(5, base-2), int(base*2), int(base*3)+8),\n",
        "    ]\n",
        "    uniq, seen = [], set()\n",
        "    for t in candidates:\n",
        "        t2 = tuple(sorted([int(max(5, w)) for w in t]))\n",
        "        if t2 not in seen:\n",
        "            uniq.append(t2); seen.add(t2)\n",
        "    return uniq\n",
        "\n",
        "def compute_feature_anomaly(df, feature_cols):\n",
        "    # anomaly per cycle via robust z of features\n",
        "    if df.empty:\n",
        "        return np.array([0.0])\n",
        "    A = np.zeros(len(df), dtype=float)\n",
        "    for col in feature_cols:\n",
        "        series = df[col].astype(float).values\n",
        "        A += np.abs(zscore(series, robust=True))\n",
        "    return A\n",
        "\n",
        "def initiation_index_multiscale(df, feature_cols, ignore_frac=0.07, min_run=6, tail_ignore_frac=0.0):\n",
        "    if df.empty:\n",
        "        return int(10**9)\n",
        "    cycles = np.array(sorted(df[\"cycle_index\"].values))\n",
        "    base = compute_feature_anomaly(df, feature_cols)\n",
        "    votes = []\n",
        "    for windows in dynamic_window_candidates(len(cycles)):\n",
        "        smoothed = []\n",
        "        for w in windows:\n",
        "            sm = smooth_centered(base, w, method=\"gaussian\")\n",
        "            sm_z = zscore(sm, robust=True)\n",
        "            smoothed.append(sm_z)\n",
        "        weights = np.array([1.0 / np.sqrt(w) for w in windows])\n",
        "        weights = weights / (weights.sum() + 1e-12)\n",
        "        anomaly_ms = np.tensordot(weights, np.vstack(smoothed), axes=(0,0))\n",
        "        baseline_len = max(5, len(anomaly_ms) // 5)\n",
        "        baseline = anomaly_ms[:baseline_len]\n",
        "        thresh = float(np.mean(baseline) + 2.0 * np.std(baseline))\n",
        "        slopes = np.diff(anomaly_ms)\n",
        "        slope_mu = float(np.mean(slopes)) if len(slopes) else 0.0\n",
        "        slope_sigma = float(np.std(slopes)) + 1e-12\n",
        "        slope_gate = slope_mu + 0.8 * slope_sigma\n",
        "        start_idx = int(len(cycles) * ignore_frac)\n",
        "        end_idx = len(cycles) - int(len(cycles) * tail_ignore_frac)\n",
        "        end_idx = max(end_idx, start_idx + min_run + 1)\n",
        "        above = anomaly_ms > thresh\n",
        "        candidates = []\n",
        "        for i in range(start_idx, end_idx - min_run):\n",
        "            sustained = np.all(above[i:i + min_run])\n",
        "            slope_ok = np.all(slopes[i:i + min_run - 1] > slope_gate) if (i + min_run - 1) <= len(slopes) else False\n",
        "            if sustained and slope_ok:\n",
        "                local_strength = float(np.sum(slopes[i:i + min_run - 1])) if (i + min_run - 1) <= len(slopes) else 0.0\n",
        "                candidates.append((cycles[i], local_strength))\n",
        "        # add top-3 anomaly indices as backup votes\n",
        "        topk = min(3, len(anomaly_ms))\n",
        "        top_idx = np.argsort(anomaly_ms)[-topk:]\n",
        "        for tidx in top_idx:\n",
        "            s_local = float(slopes[tidx-1]) if tidx-1 >= 0 and tidx-1 < len(slopes) else 0.0\n",
        "            candidates.append((cycles[tidx], max(0.0, s_local)))\n",
        "        if not candidates:\n",
        "            votes.append(int(cycles[np.argmax(anomaly_ms)]))\n",
        "        else:\n",
        "            c_cycles = np.array([c[0] for c in candidates])\n",
        "            c_weights = np.array([c[1] for c in candidates]) + 1e-6\n",
        "            c_weights = c_weights / (c_weights.sum() + 1e-12)\n",
        "            order = np.argsort(c_cycles)\n",
        "            cc = c_cycles[order]\n",
        "            ww = c_weights[order]\n",
        "            cumw = np.cumsum(ww)\n",
        "            idx = np.searchsorted(cumw, 0.5)\n",
        "            votes.append(int(cc[min(idx, len(cc)-1)]))\n",
        "    return int(np.median(votes)) if len(votes) else int(10**9)\n",
        "\n",
        "def anomaly_cluster_order(processed):\n",
        "    feature_cols = [\"rms\",\"skew\",\"kurt\",\"band_cage\",\"band_ball\",\"band_inner\",\"band_outer\"]\n",
        "    init_map = {}\n",
        "    for item_id, df in processed.items():\n",
        "        idx = initiation_index_multiscale(df, feature_cols, ignore_frac=0.07, min_run=6, tail_ignore_frac=0.0)\n",
        "        init_map[item_id] = idx\n",
        "    order = sorted(init_map.keys(), key=lambda k: (init_map[k], k))\n",
        "    return order, init_map\n",
        "\n",
        "# ============================\n",
        "# Pairwise mapping (similarity/distance)\n",
        "# ============================\n",
        "def item_feature_vector(df):\n",
        "    if df.empty:\n",
        "        return np.zeros(8, dtype=float)\n",
        "    vec = [\n",
        "        np.mean(df[\"rms\"]),\n",
        "        np.mean(df[\"skew\"]),\n",
        "        np.mean(df[\"kurt\"]),\n",
        "        np.mean(df[\"tach_mean\"]),\n",
        "        np.mean(df[\"turbine_mean\"]),\n",
        "        np.mean(df[\"band_cage\"]),\n",
        "        np.mean(df[\"band_ball\"]),\n",
        "        np.mean(df[\"band_inner\"]),\n",
        "        # to keep vector length consistent, combine outer too\n",
        "    ]\n",
        "    vec.append(np.mean(df[\"band_outer\"]))\n",
        "    return np.array(vec, dtype=float)\n",
        "\n",
        "def pairwise_map(processed):\n",
        "    ids = sorted(processed.keys())\n",
        "    M = np.zeros((len(ids), len(ids)), dtype=float)\n",
        "    vectors = {i: item_feature_vector(processed[i]) for i in ids}\n",
        "    # distance: robust z, then L1\n",
        "    all_vecs = np.vstack([vectors[i] for i in ids])\n",
        "    all_vecs_z = (all_vecs - np.median(all_vecs, axis=0)) / (1.4826 * (np.median(np.abs(all_vecs - np.median(all_vecs, axis=0)), axis=0) + 1e-12))\n",
        "    for a_idx, ia in enumerate(ids):\n",
        "        for b_idx, ib in enumerate(ids):\n",
        "            M[a_idx, b_idx] = np.sum(np.abs(all_vecs_z[a_idx] - all_vecs_z[b_idx]))\n",
        "    # export\n",
        "    dfM = pd.DataFrame(M, index=ids, columns=ids)\n",
        "    dfM.to_csv(os.path.join(DATA_DIR, \"pairwise_map.csv\"))\n",
        "    return dfM\n",
        "\n",
        "# ============================\n",
        "# Final order fusion: bell-curve + anomaly clusters + pairwise consensus\n",
        "# ============================\n",
        "def ranks_from_order(order):\n",
        "    return {item_id: rank for rank, item_id in enumerate(order, start=1)}\n",
        "\n",
        "def fuse_orders(bell_order, anomaly_order, pairwise_df):\n",
        "    ids = sorted(set(bell_order).union(set(anomaly_order)))\n",
        "    r_bell = ranks_from_order(bell_order)\n",
        "    r_anom = ranks_from_order(anomaly_order)\n",
        "    # pairwise centrality: items with lower average distance are \"central\"\n",
        "    avg_dist = {i: float(np.mean(pairwise_df.loc[i].values)) if i in pairwise_df.index else 0.0 for i in ids}\n",
        "    # normalize to ranks (lower distance -> earlier)\n",
        "    dist_order = [i for i, _ in sorted(avg_dist.items(), key=lambda kv: kv[1])]\n",
        "    r_dist = ranks_from_order(dist_order)\n",
        "    # Weighted fusion to emulate 134288 (favor anomaly, then bell, then pairwise)\n",
        "    fused_score = {i: 0.55 * r_anom.get(i, len(ids)) + 0.35 * r_bell.get(i, len(ids)) + 0.10 * r_dist.get(i, len(ids)) for i in ids}\n",
        "    final = [i for i, _ in sorted(fused_score.items(), key=lambda kv: kv[1])]\n",
        "    return final\n",
        "\n",
        "# ============================\n",
        "# Main pipeline\n",
        "# ============================\n",
        "def run_pipeline_and_submit(data_dir=DATA_DIR, out_path=\"submission.csv\"):\n",
        "    # 1) Load raw files and extract features\n",
        "    items, processed = process_all(data_dir)\n",
        "    if not processed:\n",
        "        raise RuntimeError(\"No items processed. Ensure /content has file_XX.csv with v,zct.\")\n",
        "\n",
        "    # 2) Graph presented order\n",
        "    plot_factors_presented(items, processed)\n",
        "\n",
        "    # 3) Bell-curve order from scalar scores\n",
        "    bell_order, bell_scores, bell_z = build_bell_curve_order(processed)\n",
        "    # Graph in bell order\n",
        "    plot_factors_bell(items, processed, bell_order)\n",
        "\n",
        "    # 4) Bearing-feature output\n",
        "    bf = summarize_bearing_features(processed)\n",
        "    print(\"Saved bearing_features.csv\")\n",
        "\n",
        "    # 5) Anomaly-clustered order (emulating 134288 clustering)\n",
        "    anomaly_order, init_map = anomaly_cluster_order(processed)\n",
        "\n",
        "    # 6) Pairwise mapping\n",
        "    dfM = pairwise_map(processed)\n",
        "    print(\"Saved pairwise_map.csv\")\n",
        "\n",
        "    # 7) Fuse orders into final prediction\n",
        "    final_order = fuse_orders(bell_order, anomaly_order, dfM)\n",
        "\n",
        "    # 8) Write submission\n",
        "    sub_df = pd.DataFrame({\"prediction\": final_order})\n",
        "    sub_df.to_csv(out_path, index=False)\n",
        "    print(f\"Wrote: {out_path}\")\n",
        "    print(\"First 20 predictions:\", final_order[:20])\n",
        "\n",
        "# ============================\n",
        "# Entrypoint\n",
        "# ============================\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline_and_submit(DATA_DIR, out_path=\"submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwNemiuy7MWC",
        "outputId": "73eab50b-e1d9-43ea-f86e-1bfbeb643cbb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 53 files\n",
            "Item 01 (file_01.csv): 211 cycles extracted\n",
            "Item 02 (file_02.csv): 211 cycles extracted\n",
            "Item 03 (file_03.csv): 211 cycles extracted\n",
            "Item 04 (file_04.csv): 210 cycles extracted\n",
            "Item 05 (file_05.csv): 211 cycles extracted\n",
            "Item 06 (file_06.csv): 210 cycles extracted\n",
            "Item 07 (file_07.csv): 211 cycles extracted\n",
            "Item 08 (file_08.csv): 210 cycles extracted\n",
            "Item 09 (file_09.csv): 210 cycles extracted\n",
            "Item 10 (file_10.csv): 210 cycles extracted\n",
            "Item 11 (file_11.csv): 211 cycles extracted\n",
            "Item 12 (file_12.csv): 210 cycles extracted\n",
            "Item 13 (file_13.csv): 211 cycles extracted\n",
            "Item 14 (file_14.csv): 211 cycles extracted\n",
            "Item 15 (file_15.csv): 210 cycles extracted\n",
            "Item 16 (file_16.csv): 211 cycles extracted\n",
            "Item 17 (file_17.csv): 210 cycles extracted\n",
            "Item 18 (file_18.csv): 211 cycles extracted\n",
            "Item 19 (file_19.csv): 210 cycles extracted\n",
            "Item 20 (file_20.csv): 210 cycles extracted\n",
            "Item 21 (file_21.csv): 211 cycles extracted\n",
            "Item 22 (file_22.csv): 211 cycles extracted\n",
            "Item 23 (file_23.csv): 210 cycles extracted\n",
            "Item 24 (file_24.csv): 210 cycles extracted\n",
            "Item 25 (file_25.csv): 210 cycles extracted\n",
            "Item 26 (file_26.csv): 210 cycles extracted\n",
            "Item 27 (file_27.csv): 211 cycles extracted\n",
            "Item 28 (file_28.csv): 210 cycles extracted\n",
            "Item 29 (file_29.csv): 210 cycles extracted\n",
            "Item 30 (file_30.csv): 210 cycles extracted\n",
            "Item 31 (file_31.csv): 211 cycles extracted\n",
            "Item 32 (file_32.csv): 210 cycles extracted\n",
            "Item 33 (file_33.csv): 211 cycles extracted\n",
            "Item 34 (file_34.csv): 211 cycles extracted\n",
            "Item 35 (file_35.csv): 210 cycles extracted\n",
            "Item 36 (file_36.csv): 210 cycles extracted\n",
            "Item 37 (file_37.csv): 211 cycles extracted\n",
            "Item 38 (file_38.csv): 210 cycles extracted\n",
            "Item 39 (file_39.csv): 211 cycles extracted\n",
            "Item 40 (file_40.csv): 210 cycles extracted\n",
            "Item 41 (file_41.csv): 210 cycles extracted\n",
            "Item 42 (file_42.csv): 210 cycles extracted\n",
            "Item 43 (file_43.csv): 211 cycles extracted\n",
            "Item 44 (file_44.csv): 211 cycles extracted\n",
            "Item 45 (file_45.csv): 210 cycles extracted\n",
            "Item 46 (file_46.csv): 210 cycles extracted\n",
            "Item 47 (file_47.csv): 211 cycles extracted\n",
            "Item 48 (file_48.csv): 211 cycles extracted\n",
            "Item 49 (file_49.csv): 211 cycles extracted\n",
            "Item 50 (file_50.csv): 211 cycles extracted\n",
            "Item 51 (file_51.csv): 210 cycles extracted\n",
            "Item 52 (file_52.csv): 211 cycles extracted\n",
            "Item 53 (file_53.csv): 210 cycles extracted\n",
            "Saved bearing_features.csv\n",
            "Saved pairwise_map.csv\n",
            "Wrote: submission.csv\n",
            "First 20 predictions: [30, 25, 34, 3, 16, 26, 11, 41, 44, 23, 10, 5, 7, 48, 53, 28, 36, 37, 42, 38]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair-Wise mapping of V vs ZCT"
      ],
      "metadata": {
        "id": "tZwti0A9GdXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready: relational graphs across files, pairwise V–ZCT mapping, bell-curve ordering, anomaly clustering, and final prediction\n",
        "import os, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "# ============================\n",
        "# Configuration\n",
        "# ============================\n",
        "DATA_DIR = \"/content\"       # expects file_01.csv ... file_53.csv with columns ['v','zct']\n",
        "FS = 93750.0                # acceleration sampling frequency (Hz)\n",
        "GEAR_RATIO = 5.095238095    # tachometer shaft to turbine shaft\n",
        "MEAN_TACH = 105.25          # Hz approx\n",
        "NOM_TURBINE_SPEED = MEAN_TACH * GEAR_RATIO  # ~536.27 Hz\n",
        "GEOMETRY_FACTORS = {\"cage\":0.43, \"ball\":7.05, \"inner\":10.78, \"outer\":8.22}\n",
        "FAULT_BANDS = {k: NOM_TURBINE_SPEED*v for k,v in GEOMETRY_FACTORS.items()}\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Plot output directories\n",
        "PLOT_DIR = os.path.join(DATA_DIR, \"relational_graphs\")\n",
        "os.makedirs(PLOT_DIR, exist_ok=True)\n",
        "\n",
        "# ============================\n",
        "# Helpers (robust stats, smoothing)\n",
        "# ============================\n",
        "def mad(x):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    med = np.median(x)\n",
        "    return np.median(np.abs(x - med)) + 1e-12\n",
        "\n",
        "def zscore(x, robust=False):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    if robust:\n",
        "        mu = np.median(x)\n",
        "        sigma = 1.4826 * mad(x)\n",
        "    else:\n",
        "        mu = np.mean(x)\n",
        "        sigma = np.std(x) + 1e-12\n",
        "    return (x - mu) / sigma\n",
        "\n",
        "def gaussian_smooth(arr, window, sigma=None):\n",
        "    arr = np.asarray(arr, dtype=float)\n",
        "    w = int(max(3, window))\n",
        "    if sigma is None:\n",
        "        sigma = max(1.0, w / 5.0)\n",
        "    half = w // 2\n",
        "    x = np.arange(-half, half + 1, dtype=float)\n",
        "    ker = np.exp(-(x**2) / (2 * sigma**2))\n",
        "    ker = ker / (ker.sum() + 1e-12)\n",
        "    pad = np.pad(arr, (half, half), mode='edge')\n",
        "    sm = np.convolve(pad, ker, mode='valid')\n",
        "    return sm\n",
        "\n",
        "def smooth_centered(arr, window=9):\n",
        "    return gaussian_smooth(arr, window)\n",
        "\n",
        "# ============================\n",
        "# Loader and zct cleaning\n",
        "# ============================\n",
        "def load_files(data_dir=DATA_DIR):\n",
        "    files = sorted(glob.glob(os.path.join(data_dir,\"file_*.csv\")))\n",
        "    items = {}\n",
        "    for path in files:\n",
        "        base = os.path.basename(path)\n",
        "        m = re.match(r\"file_(\\d+)\\.csv\", base)\n",
        "        if not m:\n",
        "            continue\n",
        "        item_id = int(m.group(1))\n",
        "        try:\n",
        "            df = pd.read_csv(path)\n",
        "        except Exception as e:\n",
        "            print(f\"Skip {base}: read error {e}\")\n",
        "            continue\n",
        "        if not {\"v\",\"zct\"}.issubset(df.columns):\n",
        "            print(f\"Skip {base}: missing v/zct\")\n",
        "            continue\n",
        "        df = df[[\"v\",\"zct\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
        "        # Clean zct\n",
        "        zct = df[\"zct\"].dropna().values\n",
        "        if len(zct) >= 2:\n",
        "            zct = np.sort(np.unique(zct))\n",
        "            if len(zct) >= 5:\n",
        "                zct = pd.Series(zct).rolling(5, center=True, min_periods=1).median().values\n",
        "            diffs = np.diff(zct)\n",
        "            keep = [0] + [i+1 for i in range(len(diffs)) if diffs[i] > 0]\n",
        "            zct = zct[keep]\n",
        "        if len(zct) < 2:\n",
        "            # Fallback: create coarse cycle boundaries across the recording\n",
        "            n = len(df[\"v\"])\n",
        "            approx_cycles = max(2, int(n / FS))\n",
        "            zct = np.linspace(0.0, approx_cycles, approx_cycles+1)\n",
        "        items[item_id] = {\"v\": df[\"v\"].dropna().values.astype(float),\n",
        "                          \"zct\": zct.astype(float),\n",
        "                          \"filename\": base}\n",
        "    print(f\"Loaded {len(items)} files\")\n",
        "    return items\n",
        "\n",
        "# ============================\n",
        "# Tachometer/turbine utilities\n",
        "# ============================\n",
        "def tachometer_frequency(zct):\n",
        "    delta = np.diff(zct)\n",
        "    delta = np.where(delta <= 0, np.nan, delta)\n",
        "    f = 1.0/(delta)\n",
        "    return f\n",
        "\n",
        "def turbine_speed(f_tach):\n",
        "    return f_tach * GEAR_RATIO\n",
        "\n",
        "# ============================\n",
        "# Band energy feature (for anomaly clustering)\n",
        "# ============================\n",
        "def band_energy(signal, fs, center, bw=50.0):\n",
        "    nyq = 0.5*fs\n",
        "    low_hz = max(1.0, center - bw)\n",
        "    high_hz = min(nyq - 1.0, center + bw)\n",
        "    if low_hz >= high_hz:\n",
        "        return 0.0\n",
        "    low = low_hz/nyq\n",
        "    high = high_hz/nyq\n",
        "    try:\n",
        "        b,a = butter(4,[low,high],btype='band')\n",
        "        filt = filtfilt(b,a,signal)\n",
        "        return float(np.mean(filt**2))\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# ============================\n",
        "# Per-file summary features (V and ZCT)\n",
        "# ============================\n",
        "def summarize_file(v, zct, fs=FS):\n",
        "    # V features (acceleration)\n",
        "    v_rms = float(np.sqrt(np.mean(v**2))) if len(v) > 0 else 0.0\n",
        "    v_skew = float(skew(v)) if len(v) > 10 else 0.0\n",
        "    v_kurt = float(kurtosis(v)) if len(v) > 10 else 0.0\n",
        "    # ZCT features (tach)\n",
        "    f_tach = tachometer_frequency(zct)\n",
        "    tach_mean = float(np.nanmean(f_tach)) if len(f_tach) else 0.0\n",
        "    tach_std  = float(np.nanstd(f_tach))  if len(f_tach) else 0.0\n",
        "    turbine_mean = float(np.nanmean(turbine_speed(f_tach))) if len(f_tach) else 0.0\n",
        "\n",
        "    # Optional bearing band energies from whole-record signal\n",
        "    be_cage  = band_energy(v, fs, FAULT_BANDS[\"cage\"],  bw=40.0)\n",
        "    be_ball  = band_energy(v, fs, FAULT_BANDS[\"ball\"],  bw=80.0)\n",
        "    be_inner = band_energy(v, fs, FAULT_BANDS[\"inner\"], bw=80.0)\n",
        "    be_outer = band_energy(v, fs, FAULT_BANDS[\"outer\"], bw=80.0)\n",
        "\n",
        "    return {\n",
        "        \"v_rms\": v_rms, \"v_skew\": v_skew, \"v_kurt\": v_kurt,\n",
        "        \"tach_mean\": tach_mean, \"tach_std\": tach_std, \"turbine_mean\": turbine_mean,\n",
        "        \"band_cage\": be_cage, \"band_ball\": be_ball, \"band_inner\": be_inner, \"band_outer\": be_outer\n",
        "    }\n",
        "\n",
        "def build_summary(items):\n",
        "    rows = []\n",
        "    for item_id in sorted(items.keys()):\n",
        "        v = items[item_id][\"v\"]; zct = items[item_id][\"zct\"]\n",
        "        s = summarize_file(v, zct, fs=FS)\n",
        "        s[\"item_id\"] = item_id\n",
        "        rows.append(s)\n",
        "    summary = pd.DataFrame(rows).sort_values(\"item_id\")\n",
        "    summary.to_csv(os.path.join(DATA_DIR, \"summary_v_zct.csv\"), index=False)\n",
        "    return summary\n",
        "\n",
        "# ============================\n",
        "# Pairwise mapping between files (combined V–ZCT)\n",
        "# ============================\n",
        "def pairwise_vzct(summary):\n",
        "    ids = summary[\"item_id\"].values.tolist()\n",
        "    # Feature vector combining V and ZCT summaries\n",
        "    F = summary[[\"v_rms\",\"v_skew\",\"v_kurt\",\"tach_mean\",\"tach_std\",\"turbine_mean\"]].values.astype(float)\n",
        "    # Robust z-normalization across items\n",
        "    med = np.median(F, axis=0)\n",
        "    madv = 1.4826 * (np.median(np.abs(F - med), axis=0) + 1e-12)\n",
        "    Fz = (F - med) / madv\n",
        "    # Pairwise L1 distance matrix\n",
        "    n = len(ids)\n",
        "    M = np.zeros((n,n), dtype=float)\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            M[i,j] = float(np.sum(np.abs(Fz[i] - Fz[j])))\n",
        "    dfM = pd.DataFrame(M, index=ids, columns=ids)\n",
        "    dfM.to_csv(os.path.join(DATA_DIR, \"pairwise_map_vzct.csv\"))\n",
        "    # Deviation score per item: L1 distance from the robust center (zero vector)\n",
        "    dev_scores = np.sum(np.abs(Fz), axis=1)\n",
        "    dev_df = pd.DataFrame({\"item_id\": ids, \"deviation_score\": dev_scores})\n",
        "    dev_df = dev_df.sort_values(\"item_id\")\n",
        "    dev_df.to_csv(os.path.join(DATA_DIR, \"deviation_scores.csv\"), index=False)\n",
        "    return dfM, dev_df, Fz\n",
        "\n",
        "# ============================\n",
        "# Relational graphs across files (presented vs bell-curve order)\n",
        "# ============================\n",
        "def plot_relational_graphs(summary, dev_df):\n",
        "    # Presented order: file_01 -> file_53\n",
        "    ids_presented = summary[\"item_id\"].values\n",
        "    y_v = summary[\"v_rms\"].values\n",
        "    y_z = summary[\"tach_mean\"].values\n",
        "\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(12,8), sharex=True)\n",
        "    ax[0].plot(ids_presented, smooth_centered(y_v, 9), color='steelblue', lw=1.5, label='V RMS (smoothed)')\n",
        "    ax[0].plot(ids_presented, y_v, color='lightblue', lw=0.5, alpha=0.5)\n",
        "    ax[0].set_title(\"Presented order (file_01 → file_53): V RMS\")\n",
        "    ax[0].set_ylabel(\"V RMS\")\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(ids_presented, smooth_centered(y_z, 9), color='darkorange', lw=1.5, label='Tach mean (smoothed)')\n",
        "    ax[1].plot(ids_presented, y_z, color='moccasin', lw=0.5, alpha=0.5)\n",
        "    ax[1].set_title(\"Presented order: Tachometer mean frequency\")\n",
        "    ax[1].set_xlabel(\"Item id (presented order)\")\n",
        "    ax[1].set_ylabel(\"Tach mean (Hz)\")\n",
        "    ax[1].legend()\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(PLOT_DIR, \"relational_presented.png\"))\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Bell-curve order: sort by deviation_score ascending (center → tails)\n",
        "    dev_sorted = dev_df.sort_values(\"deviation_score\")\n",
        "    ids_bell = dev_sorted[\"item_id\"].values\n",
        "    # Map summary to bell order\n",
        "    s_idx = {iid: i for i, iid in enumerate(ids_presented)}\n",
        "    y_v_bell = np.array([summary.loc[s_idx[iid], \"v_rms\"] for iid in ids_bell])\n",
        "    y_z_bell = np.array([summary.loc[s_idx[iid], \"tach_mean\"] for iid in ids_bell])\n",
        "\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(12,8), sharex=True)\n",
        "    ax[0].plot(range(1,len(ids_bell)+1), smooth_centered(y_v_bell, 9), color='seagreen', lw=1.5, label='V RMS (smoothed)')\n",
        "    ax[0].plot(range(1,len(ids_bell)+1), y_v_bell, color='palegreen', lw=0.5, alpha=0.5)\n",
        "    ax[0].set_title(\"Bell-curve order (by deviation from normal): V RMS\")\n",
        "    ax[0].set_ylabel(\"V RMS\")\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(range(1,len(ids_bell)+1), smooth_centered(y_z_bell, 9), color='purple', lw=1.5, label='Tach mean (smoothed)')\n",
        "    ax[1].plot(range(1,len(ids_bell)+1), y_z_bell, color='thistle', lw=0.5, alpha=0.5)\n",
        "    ax[1].set_title(\"Bell-curve order: Tachometer mean frequency\")\n",
        "    ax[1].set_xlabel(\"Rank in bell-curve order (center → tails)\")\n",
        "    ax[1].set_ylabel(\"Tach mean (Hz)\")\n",
        "    ax[1].legend()\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(PLOT_DIR, \"relational_bell.png\"))\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Scatter comparisons (V vs ZCT) in both orders\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14,6))\n",
        "    ax[0].scatter(y_v, y_z, c=ids_presented, cmap='viridis', s=30)\n",
        "    ax[0].set_title(\"Presented order: V RMS vs Tach mean\")\n",
        "    ax[0].set_xlabel(\"V RMS\"); ax[0].set_ylabel(\"Tach mean (Hz)\")\n",
        "\n",
        "    ax[1].scatter(y_v_bell, y_z_bell, c=range(len(ids_bell)), cmap='plasma', s=30)\n",
        "    ax[1].set_title(\"Bell-curve order: V RMS vs Tach mean\")\n",
        "    ax[1].set_xlabel(\"V RMS\"); ax[1].set_ylabel(\"Tach mean (Hz)\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(PLOT_DIR, \"scatter_v_vs_zct.png\"))\n",
        "    plt.close(fig)\n",
        "\n",
        "# ============================\n",
        "# Anomaly clustering (engineered features per file)\n",
        "# ============================\n",
        "def anomaly_score_for_order(summary):\n",
        "    # Use robust z of V RMS and band energies to emulate anomaly clustering\n",
        "    cols = [\"v_rms\",\"band_cage\",\"band_ball\",\"band_inner\",\"band_outer\"]\n",
        "    F = summary[cols].values.astype(float)\n",
        "    med = np.median(F, axis=0)\n",
        "    madv = 1.4826 * (np.median(np.abs(F - med), axis=0) + 1e-12)\n",
        "    Fz = (F - med) / madv\n",
        "    # Aggregate anomaly magnitude\n",
        "    A = np.sum(np.abs(Fz), axis=1)\n",
        "    ids = summary[\"item_id\"].values\n",
        "    order = [i for i,_ in sorted(zip(ids, A), key=lambda kv: kv[1])]  # low anomaly first\n",
        "    return order, A\n",
        "\n",
        "# ============================\n",
        "# Final order fusion and submission\n",
        "# ============================\n",
        "def ranks_from_order(order):\n",
        "    return {item_id: rank for rank, item_id in enumerate(order, start=1)}\n",
        "\n",
        "def fuse_orders(bell_order, anomaly_order):\n",
        "    ids = sorted(set(bell_order).union(set(anomaly_order)))\n",
        "    r_bell = ranks_from_order(bell_order)\n",
        "    r_anom = ranks_from_order(anomaly_order)\n",
        "    # Favor anomaly clustering to emulate 134288, but include bell-curve relational progression\n",
        "    fused_score = {i: 0.6 * r_anom.get(i, len(ids)) + 0.4 * r_bell.get(i, len(ids)) for i in ids}\n",
        "    final = [i for i,_ in sorted(fused_score.items(), key=lambda kv: kv[1])]\n",
        "    return final\n",
        "\n",
        "# ============================\n",
        "# Main pipeline\n",
        "# ============================\n",
        "def run_pipeline_and_submit(data_dir=DATA_DIR, out_path=\"submission.csv\"):\n",
        "    # Load files\n",
        "    items = load_files(data_dir)\n",
        "    if not items:\n",
        "        raise RuntimeError(\"No item CSVs loaded. Ensure /content contains file_XX.csv with v,zct.\")\n",
        "\n",
        "    # Build V–ZCT summary\n",
        "    summary = build_summary(items)\n",
        "\n",
        "    # Pairwise mapping and deviation scores\n",
        "    dfM, dev_df, Fz = pairwise_vzct(summary)\n",
        "    print(\"Saved pairwise_map_vzct.csv and deviation_scores.csv\")\n",
        "\n",
        "    # Relational graphs across files (presented vs bell-curve)\n",
        "    plot_relational_graphs(summary, dev_df)\n",
        "    print(f\"Saved relational graphs to {PLOT_DIR}\")\n",
        "\n",
        "    # Bell-curve order (center-to-tails by deviation from normal)\n",
        "    bell_order = dev_df.sort_values(\"deviation_score\")[\"item_id\"].tolist()\n",
        "\n",
        "    # Anomaly-clustered order (bearing-feature emphasis)\n",
        "    anom_order, A = anomaly_score_for_order(summary)\n",
        "\n",
        "    # Fuse into final prediction\n",
        "    final_order = fuse_orders(bell_order, anom_order)\n",
        "\n",
        "    # Write submission\n",
        "    sub_df = pd.DataFrame({\"prediction\": final_order})\n",
        "    sub_df.to_csv(out_path, index=False)\n",
        "    print(f\"Wrote: {out_path}\")\n",
        "    print(\"First 20 predictions:\", final_order[:20])\n",
        "\n",
        "# ============================\n",
        "# Entrypoint\n",
        "# ============================\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline_and_submit(DATA_DIR, out_path=\"submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quKh41RYGcV3",
        "outputId": "911bb544-ee17-45e9-b394-6bf14e066c90"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 53 files\n",
            "Saved pairwise_map_vzct.csv and deviation_scores.csv\n",
            "Saved relational graphs to /content/relational_graphs\n",
            "Wrote: submission.csv\n",
            "First 20 predictions: [9, 11, 4, 12, 1, 10, 18, 7, 6, 17, 2, 3, 19, 16, 5, 23, 26, 8, 30, 27]\n"
          ]
        }
      ]
    }
  ]
}